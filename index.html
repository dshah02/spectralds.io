<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="SpectraLDS: Provable Distillation for Linear Dynamical Systems" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
  <link rel="stylesheet" href="static/css/index.css" />

  <title>SpectraLDS: Provable Distillation for Linear Dynamical Systems</title>

  <script type="text/javascript" id="MathJax-script" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
  <script>
    function selectContent(query) {
      var range = document.createRange()
      var selection = window.getSelection()
      var elem = document.querySelector(query)
      range.selectNodeContents(elem)
      selection.removeAllRanges()
      selection.addRange(range)
    }
  </script>
</head>

<body>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
        displayMath: [
          ["$$", "$$"],
          ["\\[", "\\]"],
        ],
        processEscapes: true,
        macros: {
          planclass: "\\operatorname{\\mathbf{plan}}",
          S: "\\mathcal{S}",
          gS: "\\mathcal{S}",
          cS: "\\mathcal{S}",
          cB: "\\mathcal{B}",
          cA: "\\mathcal{A}",
          A: "\\mathcal{A}",
          gA: "\\mathcal{A}",
          D: "\\mathcal{D}",
          R: "\\mathbb{R}",
          E: "\\mathbb{E}",
          P: "\\mathrm{P}",
          var: "\\mathrm{Var}",
          cov: "\\mathrm{Cov}",
          argmin: "\\mathop{\\arg\\min}",
          argmax: "\\mathop{\\arg\\max}",
        },
      },
      svg: {
        fontCache: "global",
      },
    }
  </script>
  <header>
    <h1>SpectraLDS: Provable Distillation for Linear Dynamical Systems</h1>
    <div class="authors">
      <span class="author">
        <a>Devan Shah</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Shlomo Fortgang</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Sofiia Druchnya</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Elad Hazan</a></span>
      <span class="affil">1,2</span>
    </div>
    <div class="notes">
      <span class="affil">1</span>
      <span class="institution">Princeton University</span>
      <span class="affil">2</span>
      <span class="institution">Google DeepMind Princeton</span>
    </div>
    <div class="links">
      <span class="link">
        <a href="./static/pdf/SpectraLDS.pdf" target="_blank" class="button">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
      </span>
      <span class="link">
        <a href="https://arxiv.org/pdf/2505.17868" class="button">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>
      </span>
      <span class="link">
        <a href="https://github.com/dshah02/SpectraLDS" class="button">
          <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
        </a>
      </span>
    </div>
  </header>

  <main>
    <section>
      <div class="summary">
        <div class="image-grid">
          <div class="image-group">
            <h4>Spectral Filters</h4>
            <div class="image-container">
              <img src="static/figures/SF.png" alt="Spectral Filters">
            </div>
          </div>

          <div class="image-group">
            <h4>LDS Impulses</h4>
            <div class="image-container">
              <img src="static/figures/LDS_impulses.png" alt="LDS Impulses">
            </div>
          </div>

          <div class="image-group">
            <h4>Reconstructed Spectral Filters</h4>
            <div class="image-container">
              <img src="static/figures/SF-recon.png" alt="Reconstructed Spectral Filters">
            </div>
          </div>
        </div>

        <style>
          .image-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 30px;
            width: 110%;
            margin-top: 70px;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
          }

          .image-group {
            text-align: center;
          }

          .image-group h4 {
            margin: 0 0 15px 0;
            color: #27579a;
            font-size: 1.1em;
          }

          .image-container {
            width: 100%;
            aspect-ratio: 1.;
            overflow: hidden;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
          }

          .image-container img {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 8px;
          }

          /* Responsive design */
          @media (max-width: 768px) {
            .image-grid {
              grid-template-columns: 1fr;
              gap: 20px;
              margin-top: 40px;
            }
            
            .image-container {
              max-width: 400px;
              max-height: 250px;
              margin: 0 auto;
            }
          }
        </style>
      </div>
    </section>

    <section>
      <div class="abstract">
        <h3>Abstract</h3>
        <p>
          We present the first provable method for identifying symmetric linear dynamical
          systems (LDS) with accuracy guarantees that are independent of the system's
          state dimension or effective memory. Our approach builds upon recent work that
          represents symmetric LDSs as convolutions learnable via fixed spectral transfor-
          mations. We show how to invert this representation—recovering an LDS model
          from its spectral transform—yielding an end-to-end convex optimization procedure.
          This distillation preserves predictive accuracy while enabling constant-time and
          constant-space inference per token, independent of sequence length. We evaluate
          our method, SpectraLDS, as a component in sequence prediction architectures and
          demonstrate that accuracy is preserved while inference efficiency is improved on
          tasks such as language modeling.
        </p>
      </div>
    </section>
    <section>
      <h2>Background: The Inference Speed Challenge</h2>
      
      <!-- First three items without boxes -->
      <div class="background-content">
        <div class="background-item">
          <h3><span class="number">1</span> The Quadratic Bottleneck</h3>
          <p>
            Transformer architectures suffer from quadratic complexity in sequence length due to self-attention, making them computationally expensive for long sequences. This has driven research toward more efficient alternatives that preserve expressiveness while reducing computational costs. To generate to generate K tokens with a prompt of length T, we require:
          </p>
          <div class="equation">
            $$\text{Attention: } O(T^2 + TK + K^2) \text{ operations}$$ 
          </div>
        </div>
        
        <div class="background-item">
          <h3><span class="number">2</span> State-Space Model Promise</h3>
          <p>
            Linear Dynamical Systems and state-space models offer attractive O(L) recurrent inference, but traditional gradient-based training suffers from instability due to vanishing/exploding gradients, especially for systems requiring long-term memory. For a RNN/LDS with state dimension h, we require:
          </p>
          <div class="equation">
            $$\text{RNN/LDS: } O(h \cdot (T + K)) \text{ operations}$$
          </div>
        </div>
        
        <div class="background-item">
          <h3><span class="number">3</span> Spectral Methods Trade-off</h3>
          <p>
            Recent spectral filtering approaches like the STU provide stable convex training for LDS-like systems, achieving impressive results on long-context tasks. However, they still require expensive convolution operations with complex algorithms (FutureFill) during inference.
          </p>
          <div class="equation">
            $$\text{STU: } O(T \log T + K \log^3 K) \text{ operations}$$
          </div>
        </div>
      </div>
      
      <!-- Fourth item in a box with constrained table -->
      <div class="insight-block">
        <h3>What we offer!</h3>
        <p>
          <strong>SpectraLDS bridges this gap</strong> by enabling stable spectral training followed by distillation to efficient LDS inference. We achieve the training robustness of spectral methods with the inference speed of recurrent models—reducing complexity from O(L log³ L) to O(L) per token while maintaining identical accuracy.
        </p>
        <div class="comparison-container">
          <table class="comparison-table">
            <thead>
              <tr>
                <th>Method</th>
                <th>Training Stability</th>
                <th>Inference Speed</th>
                <th>Memory Usage</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Transformers</td>
                <td>✓ Stable</td>
                <td>✗ O(T²)</td>
                <td>✗ O(T + K)</td>
              </tr>
              <tr>
                <td>Direct LDS</td>
                <td>✗ Unstable</td>
                <td>✓ O(L)</td>
                <td>✓ O(h)</td>
              </tr>
              <tr>
                <td>STU</td>
                <td>✓ Stable</td>
                <td>~ O(L log³ L)</td>
                <td>~ O(K)</td>
              </tr>
              <tr class="highlight-row">
                <td><strong>SpectraLDS</strong></td>
                <td>✓ Stable</td>
                <td>✓ O(L)</td>
                <td>✓ O(h)</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>
    
    <style>
    .background-content {
      margin: 30px 0;
    }
    
    .background-item {
      margin-bottom: 40px;
      padding-left: 0;
      border-left: 4px solid #27579a;
      padding-left: 25px;
      transition: all 0.3s ease;
    }
    
    .background-item:hover {
      border-left-color: #1e4a7a;
      padding-left: 30px;
    }
    
    .background-item h3 {
      color: #27579a;
      margin-top: 0;
      margin-bottom: 15px;
      display: flex;
      align-items: center;
      gap: 15px;
    }
    
    .number {
      background-color: #27579a;
      color: white;
      width: 30px;
      height: 30px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 14px;
      font-weight: bold;
      flex-shrink: 0;
    }
    
    .background-item p {
      margin-bottom: 15px;
      line-height: 1.6;
      color: #444;
    }
    
    .insight-block {
      background-color: #f0f4f8;
      border: 2px solid #27579a;
      border-radius: 12px;
      padding: 30px;
      margin: 40px 0;
    }
    
    .insight-block h3 {
      color: #27579a;
      margin-top: 0;
      margin-bottom: 20px;
      font-size: 1.3em;
    }
    
    .insight-block p {
      margin-bottom: 25px;
      line-height: 1.6;
    }
    
    .comparison-container {
      width: 100%;
      max-width: 100%;
      overflow-x: auto;
      margin: 0;
    }
    
    .comparison-table {
      width: 100%;
      max-width: 100%;
      border-collapse: collapse;
      background-color: white;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
      font-size: 0.9em;
    }
    
    .comparison-table th,
    .comparison-table td {
      padding: 12px 8px;
      text-align: left;
      border-bottom: 1px solid #eee;
      word-wrap: break-word;
    }
    
    .comparison-table th {
      background-color: #27579a;
      color: white;
      font-weight: 600;
      font-size: 0.85em;
    }
    
    .comparison-table td:first-child {
      font-weight: 500;
      min-width: 100px;
    }
    
    .highlight-row {
      background-color: #e8f4fd;
      font-weight: 500;
    }
    
    .highlight-row td {
      border-bottom: 2px solid #27579a;
    }
    
    .equation {
      background-color: #ffffff;
      border: 1px solid #e0e0e0;
      border-radius: 6px;
      padding: 15px;
      margin: 15px 0 0 0;
      text-align: center;
      font-size: 1.0em;
      box-shadow: 0 2px 4px rgba(0,0,0,0.05);
      max-width: 100%;
      overflow-x: auto;
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
      .background-item {
        padding-left: 20px;
        border-left-width: 3px;
      }
      
      .background-item:hover {
        padding-left: 23px;
      }
      
      .number {
        width: 25px;
        height: 25px;
        font-size: 12px;
      }
      
      .insight-block {
        padding: 20px;
        margin: 30px 0;
      }
      
      .comparison-table {
        font-size: 0.8em;
      }
      
      .comparison-table th,
      .comparison-table td {
        padding: 8px 6px;
      }
      
      .comparison-table th {
        font-size: 0.75em;
      }
    }
    
    @media (max-width: 600px) {
      .comparison-table th,
      .comparison-table td {
        padding: 6px 4px;
        font-size: 0.75em;
      }
    }
    </style>
    <section>
      <h2>Our Approach: SpectraLDS Distillation</h2>
     <div class="building-blocks-grid">
     <div class="block">
     <h3>1. Linear Dynamical Systems Primer</h3>
     <p>
      Linear Dynamical Systems (LDS) serve as the fundamental building block for state-space models used in sequence modeling. An LDS maintains a hidden state that evolves over time, enabling efficient inference while capturing long-range dependencies. However, direct gradient-based learning suffers from exploding gradients for systems with high effective memory.
     </p>
     <div class="equation">
     $$x_t = Ax_{t-1} + Bu_t, \quad \hat{y}_t = Cx_t + Du_t$$
     </div>
     </div>
     <div class="block">
     <h3>2. LDS as Convolutions</h3>
     <p>
      Any LDS can be expanded into an equivalent convolutional form by unrolling the recurrence relation. This reveals that the LDS output is actually a convolution between the input sequence and the system's impulse response, where each output is a weighted sum of past inputs.
     </p>
     <div class="equation">
     $$y_t = \sum_{i=0}^{t-1} CA^iB \cdot u_{t-i} + Du_t$$
     $$= \langle CA^0B, CA^1B, \ldots \rangle * u_{1:t}$$
     </div>
     </div>
     <div class="block">
     <h3>3. Spectral Basis Representation</h3>
     <p>
      The Spectral Transform Unit (STU) represents LDS-like dynamics using coefficients on fixed convolutional filters rather than learning the transition matrix A directly. Any LDS with symmetric A can be approximated using a combination of spectral basis functions, providing stable training with exponential approximation guarantees.
     </p>
     <div class="equation">
     $$y_t^{SF} = \sum_{j=1}^{k} M_{\varphi_j}^+ U_{t-2,j}^+ + \sum_{j=1}^{k} M_{\varphi_j}^- U_{t-2,j}^-$$ 
     </div>
     </div>
     <div class="block">
     <h3>4. Spectral to LDS Distillation</h3>
     <p>
      Our key insight is that each spectral filter can be approximated as a linear combination of LDS impulse responses. This enables converting the learned spectral representation back into explicit LDS form with provable bounded approximation error, bridging stable spectral training with efficient recurrent inference.
     </p>
     <div class="equation">
     $$\|\Phi_{1:k} - M_f \mu_L(\alpha_1,\ldots,\alpha_h)\| \leq c \lambda_{\max} h e^{-k/\log L}$$
     </div>
     </div>
     <div class="block full-width" style="margin-top: 0;">
     <div style="display: flex; align-items: center; gap: 20px;">
     <div style="flex: 2; margin: 0;">
     <h3 style="margin-top: 0;">5. Direct Architectural Substitution</h3>
     <p style="margin: 0;">
      The distillation enables replacing STU layers with equivalent LDS layers that maintain the same accuracy while achieving constant-time per-token generation. This provides the training stability of spectral methods with the inference efficiency of recurrent models, reducing computational complexity from O(L log³ L) to O(L) per token.
     </p>
     <div class="equation">
     $$U_{t,j}^+ \approx M_{fj} \sum_{i=0}^{t-1} A^i \vec{1} u_{t-i}^{\top} = \text{LDS}(M_{fj}, A, \vec{1})$$
     </div>
     </div>
     <div style="flex: 1; text-align: center;">
     <img src="static/figures/flash_stu_arch.png" style="width: 70%; border-radius: 10px; margin: 0px;" />
     </div>
     </div>
     </div>
     </div>
     <!-- Add MathJax for LaTeX rendering -->
     <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
     <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
     <script>
     window.MathJax = {
       tex: {
         inlineMath: [['$', '$'], ['\\(', '\\)']],
         displayMath: [['$$', '$$'], ['\\[', '\\]']]
       }
     };
     </script>
     <style>
     .building-blocks-grid {
     display: grid;
     grid-template-columns: repeat(2, 1fr);
     gap: 20px;
     margin: 20px 0;
     perspective: 1000px;
      }
     .block {
     background-color: #f5f5f5;
     border: 1px solid #ddd;
     border-radius: 8px;
     padding: 20px;
     transition: all 0.3s ease;
     position: relative;
     overflow: hidden;
     transform-style: preserve-3d;
      }
     .block h3 {
     color: #27579a;
     margin-top: 0;
      }
     .block p {
     margin-bottom: 0;
      }
     .equation {
     background-color: #ffffff;
     border: 1px solid #e0e0e0;
     border-radius: 6px;
     padding: 15px;
     margin: 15px 0 0 0;
     text-align: center;
     font-size: 1.1em;
     box-shadow: 0 2px 4px rgba(0,0,0,0.1);
     }
     .full-width {
     grid-column: 1 / -1;
     margin-top: 20px;
      }
     /* Add hover effects */
     .block:hover {
     transform: translateY(-5px);
     box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
      }
     h3#qualitative-evaluation {
     scroll-margin-top: 20px;
     text-align: center;
     color: #27579a;
      }
     </style>
     </section>

      <script>
        document.addEventListener('DOMContentLoaded', () => {
          const blocks = document.querySelectorAll('.block');

          blocks.forEach(block => {
            block.addEventListener('mousemove', (e) => {
              const rect = block.getBoundingClientRect();
              const x = e.clientX - rect.left;
              const y = e.clientY - rect.top;

              const centerX = rect.width / 2;
              const centerY = rect.height / 2;

              // Special case for full-width block - reduce rotation by half
              const divisor = block.classList.contains('full-width') ? 1500 : 320;
              const rotateX = -(y - centerY) / divisor;
              const rotateY = (x - centerX) / divisor;

              block.style.transform = `
                                perspective(1000px)
                                rotateX(${rotateX}deg)
                                rotateY(${rotateY}deg)
                                scale3d(1.005, 1.005, 1.005)
                            `;

              block.style.boxShadow = `
                                ${-rotateY / 16}px ${rotateX / 16}px 20px rgba(39, 87, 207, 0.15),
                                ${-rotateY / 32}px ${rotateX / 32}px 10px rgba(39, 87, 207, 0.11)
                            `;
            });

            block.addEventListener('mouseleave', () => {
              block.style.transform = 'perspective(1000px) rotateX(0) rotateY(0) scale3d(1, 1, 1)';
              block.style.boxShadow = 'none';
            });
          });
        });
      </script>
    </section>

    <section>
      <h2>Empirical Results</h2>
    
      <div class="yspace centered">
        <h3 id="runtime-analysis">Online Learning of Symmetric LDS</h3>
        <div class="full-width margin"
          style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center;">
          <div style="width: 90%; margin: 0; text-align: center;">
            <img src="static/figures/symmetric_lds_comparison.png" width="100%" />
          </div>
        </div>
        <p class="caption" style="width: 90%; margin: 20px auto; margin-top: 0px;">
          <b>SpectraLDS outperforms existing methods on synthetic LDS learning tasks</b>
          with and without noise. Our approach demonstrates superior sample efficiency and reconstruction accuracy compared to direct LDS training, attention-based methods, and other baselines. The shaded regions show 95% confidence intervals over 8 runs, highlighting the consistent performance advantages of our spectral distillation approach across different noise conditions.
        </p>
      </div>
    
      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />
    
      
      <div class="yspace centered">
        <h3 id="runtime-analysis">Constant-Time Generation: From Quadratic to Linear Scaling</h3>
        <!-- Single row for runtime comparison -->
        <div class="full-width margin" style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center;">
          <div style="width: 90%; margin: 0; text-align: center;">
            <img src="static/figures/runtime_comparison_final.png" width="100%" />
          </div> 
          </div>
          
          <p class="caption" style="width: 90%; margin-left: 5%;">
            <b>SpectraLDS achieves linear scaling in sequence length:</b><br />
            The naive convolution approach exhibits quadratic growth, FutureFill variants show logarithmic growth, while our distilled STU-to-LDS layers maintain near linear growth.  SpectraLDS also maintains nearly identical runtime regardless of LDS state dimension, indicating that LDS operations are not a computational bottleneck.
          </p>
        </div>
    
        <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />

        <!-- Language model performance comparison -->
        <h3 id="runtime-analysis">Performance of FlashSTU after LDS-for-STU substitution</h3>
        <div class="full-width margin" style="width: 100%; margin: 20px 0; display: flex; align-items: center; justify-content: center;">
          
          <div style="width: 85%; margin: 0;">
            <div class="comparison-container">
              <table class="performance-table">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>MMLU</th>
                    <th>HellaSwag</th>
                    <th>PIQA</th>
                    <th>BoolQ</th>
                    <th>Winogrande</th>
                    <th>CSQA</th>
                    <th>OBQA</th>
                    <th>ARC-e</th>
                    <th>ARC-c</th>
                    <th>Average</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>Flash STU 340M</strong></td>
                    <td>26.58</td>
                    <td>30.46</td>
                    <td>65.34</td>
                    <td>60.12</td>
                    <td>51.85</td>
                    <td>20.48</td>
                    <td>20.60</td>
                    <td>54.08</td>
                    <td>23.29</td>
                    <td>39.20</td>
                  </tr>
                  <tr class="highlight-row">
                    <td><strong>SpectraLDS 340M</strong></td>
                    <td>26.58</td>
                    <td>30.45</td>
                    <td>65.29</td>
                    <td>60.12</td>
                    <td>50.99</td>
                    <td>20.15</td>
                    <td>20.20</td>
                    <td>54.17</td>
                    <td>23.29</td>
                    <td>39.03</td>
                  </tr>
                  <tr>
                    <td><strong>Transformer 340M</strong></td>
                    <td>26.81</td>
                    <td>30.41</td>
                    <td>64.64</td>
                    <td>61.10</td>
                    <td>51.62</td>
                    <td>19.98</td>
                    <td>18.80</td>
                    <td>55.47</td>
                    <td>21.84</td>
                    <td>38.96</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p class="caption" style="margin-top: 15px; text-align: center;">
              <b>Language model performance remains identical after distillation.</b> 
              Despite converting convolution-based spectral filters into an explicit LDS formulation, 
              performance across multiple language benchmarks is equivalent, 
              demonstrating that our distillation preserves model accuracy while achieving significant inference speedups.
            </p>
          </div>
        </div>
      </div>
    
      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />
    
      <h3 id="runtime-analysis">Spectral Filter Reconstruction Quality</h3>
      <div class="yspace centered">
        <div class="full-width margin" 
             style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center; gap: 20px;">
          <div style="width: 60%; margin: 0; text-align: center;">
            <img src="static/figures/filters_reconstructed.png" width="100%" />
          </div>
          <p class="caption" style="width: 35%; margin: 0;">
            <b>High-fidelity reconstruction of spectral filters.</b><br />
            Our distillation algorithm accurately reconstructs the original spectral filters using an LDS with state dimension 80. The visualization shows both positive spectral filters (red) and their LDS approximations (blue), demonstrating excellent agreement across all 24 filters.
            <br /><br />
            <b>Reconstruction error:</b> The LDS achieves a reconstruction MSE of 1.23 × 10⁻¹², confirming that our theoretical guarantees translate to practical high-precision approximations.
          </p>
        </div>
      </div>
    </section>
    
    <section>
      <h2>Summary of Key Empirical Findings</h2>
      <div class="findings-container">
        <ul class="key-findings">
          <li>
            SpectraLDS achieves the first provable distillation from spectral filters to LDS with bounded approximation error, maintaining identical accuracy while reducing inference complexity.
          </li>
          <li>
            Our method enables constant-time per-token generation (O(L) vs O(L log³ L)) for LDS models, providing significant speedups for long sequence generation without sacrificing model performance.
          </li>
          <li>
            Language model benchmarks demonstrate that distilled models maintain performance across multiple evaluation tasks.
          </li>
          <li>
            Spectral filter reconstruction accurate reconstructs the filters, validating our theoretical approximation bounds in practice.
          </li>
          <li>
            The distillation enables hybrid architectures that combine stable spectral training with efficient recurrent inference, bridging the gap between training robustness and inference speed.
          </li>
          <li>
            Our approach scales independent of LDS state dimension and effective memory, making it feasible for challenge sequence-to-sequence prediction tasks.
          </li>
        </ul>
      </div>
    
      <style>
        .performance-table {
          width: 100%;
          border-collapse: collapse;
          background-color: white;
          border-radius: 8px;
          overflow: hidden;
          box-shadow: 0 4px 12px rgba(0,0,0,0.1);
          font-size: 0.85em;
        }
    
        .performance-table th,
        .performance-table td {
          padding: 10px 8px;
          text-align: center;
          border-bottom: 1px solid #eee;
          word-wrap: break-word;
        }
    
        .performance-table th {
          background-color: #27579a;
          color: white;
          font-weight: 600;
          font-size: 0.8em;
        }
    
        .performance-table td:first-child {
          text-align: left;
          font-weight: 500;
          min-width: 120px;
        }
    
        .highlight-row {
          background-color: #e8f4fd;
          font-weight: 500;
        }
    
        .highlight-row td {
          border-bottom: 2px solid #27579a;
        }
    
        .comparison-container {
          width: 100%;
          max-width: 100%;
          overflow-x: auto;
          margin: 0;
        }
    
        .findings-container {
          background-color: #f5f5f5;
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 25px 30px;
          margin: 20px 0;
          width: 100%;
          box-sizing: border-box;
        }
    
        .key-findings {
          margin: 0;
          padding-left: 25px;
          columns: 2;
          column-gap: 40px;
        }
    
        .key-findings li {
          margin-bottom: 15px;
          break-inside: avoid;
        }
    
        .key-findings li:last-child {
          margin-bottom: 0;
        }
    
        /* Responsive design */
        @media (max-width: 768px) {
          .performance-table {
            font-size: 0.7em;
          }
          
          .performance-table th,
          .performance-table td {
            padding: 6px 4px;
          }
          
          .key-findings {
            columns: 1;
          }
        }
      </style>
    </section>

    <section>
      <h2>
        ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em
        T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$
      </h2>
      <!--  <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@inproceedings{myers2025horizon,
    author    = {Myers, Vivek and Ji, Catherine and Eysenbach, Benjamin},
    booktitle = {{International Conference} on {Learning Representations}},
    title     = {{Horizon Generalization} in {Reinforcement Learning}},
    url       = {https://arxiv.org/abs/2501.02709},
    year      = {2025}, -->
      <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@article{shah2025spectralds,
        title={SpectraLDS: Provable Distillation for Linear Dynamical Systems}, 
        author={Devan Shah and Shlomo Fortgang and Sofiia Druchyna and Elad Hazan},
        year={2025},
        url={https://arxiv.org/abs/2505.17868}, 
  }</code></pre>
    </section>
  </main>
  <p> This template was largely forked from <a href="https://wang-kevin3290.github.io/scaling-crl/"> Kevin Wang's research site</a>.</p>
</body>

</html>
    
  </main>
</body>

</html>
