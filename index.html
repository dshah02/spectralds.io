<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description"
    content="1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.1.0/prism-bibtex.min.js"></script>
  <link rel="stylesheet" href="static/css/index.css" />

  <title>1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</title>

  <script type="text/javascript" id="MathJax-script" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg.js"></script>
  <script>
    function selectContent(query) {
      var range = document.createRange()
      var selection = window.getSelection()
      var elem = document.querySelector(query)
      range.selectNodeContents(elem)
      selection.removeAllRanges()
      selection.addRange(range)
    }
  </script>
</head>

<body>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
        displayMath: [
          ["$$", "$$"],
          ["\\[", "\\]"],
        ],
        processEscapes: true,
        macros: {
          planclass: "\\operatorname{\\mathbf{plan}}",
          S: "\\mathcal{S}",
          gS: "\\mathcal{S}",
          cS: "\\mathcal{S}",
          cB: "\\mathcal{B}",
          cA: "\\mathcal{A}",
          A: "\\mathcal{A}",
          gA: "\\mathcal{A}",
          D: "\\mathcal{D}",
          R: "\\mathbb{R}",
          E: "\\mathbb{E}",
          P: "\\mathrm{P}",
          var: "\\mathrm{Var}",
          cov: "\\mathrm{Cov}",
          argmin: "\\mathop{\\arg\\min}",
          argmax: "\\mathop{\\arg\\max}",
        },
      },
      svg: {
        fontCache: "global",
      },
    }
  </script>
  <header>
    <h1>1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities</h1>
    <div class="authors">
      <span class="author">
        <a>Kevin Wang</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Ishaan Javali</a></span>
      <span class="affil">1</span>
      <span class="author">
        <a>Michał Bortkiewicz</a></span>
      <span class="affil">2</span>
      <span class="author">
        <a>Tomasz Trzcinski</a></span>
      <span class="affil">2</span>
      <span class="author">
        <a href="https://ben-eysenbach.github.io">Benjamin Eysenbach</a></span>
      <span class="affil">1</span>
    </div>
    <div class="notes">
      <span class="affil">1</span>
      <span class="institution">Princeton University</span>
      <span class="affil">2</span>
      <span class="institution">Warsaw University of Technology</span>
    </div>
    <div class="links">
      <span class="link">
        <a href="./static/pdf/scaling_rl.pdf" target="_blank" class="button">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
      </span>
      <span class="link">
        <a href="https://arxiv.org/abs/2501.02709" class="button">
          <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
        </a>
      </span>
      <span class="link">
        <a href="https://anonymous.4open.science/r/scaling-crl-04B6/" class="button">
          <span class="icon"><i class="fas fa-code"></i></span><span>Code</span>
        </a>
      </span>
    </div>
  </header>

  <main>
    <section>
      <div class="summary">
        <!-- <video id="overview" controls="" width="99%" poster="./static/videos/overview.png">
          <source src="static/videos/overview.mp4" type="video/mp4" />
        </video> -->
        <img src="static/figures/main_results.png" width="99%" alt="Main Results" />
      </div>
    </section>

    <section>
      <div class="columns">
        <div class="wide">
          <div class="abstract">
            <h3>Abstract</h3>
            <p>
              Scaling up self-supervised learning has driven
              breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement
              learning (RL). In
              this paper, we introduce a self-supervised RL paradigm that unlocks
              substantial improvements in scalability, with network depth serving as a critical factor. Whereas
              most RL papers in recent years have relied on
              shallow architectures (around 2 – 5 layers), we
              demonstrate that increasing the depth up to 512
              layers can significantly boost performance. Our
              experiments are conducted in an unsupervised
              goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must
              explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals.
              Evaluated on a diverse range of simulated locomotion and manipulation tasks, our approach
              ranges from doubling performance to over 50×
              on humanoid-based tasks. Increasing the model
              depth not only increases success rates but also
              qualitatively changes the behaviors learned, with
              more sophisticated behaviors emerging as model
              capacity grows.
            </p>
          </div>
        </div>
        <div></div>
        <div class="medium vertical">
          <video id="overview" width="100%" autoplay loop muted poster="./static/videos/teaser.png">
            <source src="static/videos/teaser.mp4" type="video/mp4" />
          </video>
        </div>
      </div>
    </section>

    <section>
      <h2>The Building Blocks of RL Scaling (Our Approach)</h2>
      <p>
        In NLP and Computer Vision, scaling up model size has driven notable AI advancements in recent years. Yet, why hasn't RL seen comparable progress? 
        In this work, we ask: can we leverage insights and lessons from language and vision to serve as building blocks for scaling RL?
      </p>

      <p>
        At first glance, it makes sense why training very large RL networks should be difficult: the RL problem provides very few bits of feedback (e.g., only a sparse reward after a long sequence of observations), so the ratio of feedback to parameters is very small. The conventional wisdom (which many recent models reflect) has been that large AI systems must be trained primarily in a self-supervised fashion and that RL should only be used to finetune these models. Indeed, many of the recent breakthroughs in other fields have been primarily achieved with <em>self-supervised</em> methods.
      </p>

      <h3>Self-supervised learning</h3>
      <p>
        Our first step is to rethink the conventional wisdom above: "reinforcement learning" and "self-supervised learning" are not diametric learning rules, but rather can be married together into self-supervised RL systems that explore and learn policies without reference to a reward function or demonstrations. In this work, we use one of the simplest self-supervised RL algorithms, contrastive RL (CRL).
      </p>

      <h3>Data Scale</h3>
      <p>
        A key factor in scaling has been the vast amount of internet data available for learning&mdash;meanwhile, data sparsity has often been a challenge in RL. In recent years, however, there's been a proliferation of GPU-accelerated RL environments, enabling the collection of hundreds of millions of environment steps of online RL data within a few hours. In this work, we leverage the JaxGCRL environment of robotic locomotion, navigation, and manipulation tasks.
      </p>

      <h3>Signal Density (revisiting the "bits of information")</h3>
      <p>
        The success of scaling relies not only on data quantity, but also the density of training signals: in next-word prediction, every token in a text becomes a labeled example. By contrast, RL often relies on sparse, delayed rewards, with goal-conditioned RL providing only a single bit of reward feedback per trajectory. 
        However, one underlying mechanism of the CRL algorithm is Hindsight Experience Replay (HER). HER "relabels" each trajectory with the goal the agent actually achieved, so all trajectory data, even unsuccessful attempts that did not reach the goal reward, effectively become labeled samples for learning, significantly improving signal density in a self-supervised manner.
      </p>

      <h3>Classification vs. Regression</h3>
      <p>
        Scaling in language and vision are based on a classification paradigm, where increased network capacity yields predictably improved cross-entropy loss. One related work is <a href="https://arxiv.org/pdf/2403.03950">Farebrother et al. (2024)</a>, who showed that discretizing the TD objective of value-based RL into a categorical cross-entropy loss leads to improved scaling properties. 
        In this vein, note that the CRL algorithm in our approach effectively uses a cross entropy loss as well. Its InfoNCE objective is a generalization of the cross-entropy loss, thereby performing RL tasks by classifying whether current states and actions belong to the same or different trajectory that leads toward a goal state. As such, our work serves as a second piece of evidence that classification, much like cross-entropy's role in the scaling success in NLP, could serve as a building block in RL.
      </p>

      <h3>Network Architecture</h3>
      <p>
        Finally, training large RL networks often yields training instabilities. Thus, scaling will require incorporating architectural techniques from prior work, including residual connections, layer normalization, and Swish activation. These components enable increasing model capacity while preserving training stability.
      </p>

      <p>
        The primary contribution of this work is to show that integrating these ``building block'' scaling components into a single RL approach exhibits strong scalability. We anticipate that future research may build on this foundation by incorporating or uncovering additional ``building block'' elements. Below, we present the empirical results.
      </p>
    </section>

    <section>
      <h2>Empirical Results</h2>

      <div class="yspace centered">
        <div class="full-width margin"
          style="width: 100%; margin: 0; display: flex; align-items: center; justify-content: center;">
          <div style="width: 90%; margin: 0; text-align: center;">
            <img src="static/figures/main_results.png" width="100%" />
          </div>
        </div>
        <p class="caption" style="width: 90%; margin: 20px auto;">
          <b>Figure.</b> <b>Scaling network depth yields performance gains</b><br />
          across a suite of locomotion, navigation, and manipulation tasks, ranging from doubling performance to 50×
          improvements on Humanoid-based tasks. Notably,
          rather than scaling smoothly, performance often jumps at specific "critical" depths (e.g., 8 layers on Ant
          Big Maze, 64 on Humanoid U-Maze), which correspond to the emergence of qualitatively distinct learned
          <!-- policies (see <a href="#qualitative-evaluation">Qualitative Evaluation of Scaling Depth</a>). -->
          policies (see the <a href="#qualitative-evaluation">section below</a>).
        </p>
      </div>

      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />

      <h3 id="qualitative-evaluation" style="text-align: center; color: #27579a;">Scaling Depth Unlocks
        New/Qualitatively Different/Emergent
        Capabilities</h3>
      <div class="yspace centered">
        <!-- Single row for emergent capabilities -->
        <div class="full-width margin" style="width: 100%; margin: 0; display: flex; align-items: center;">
          <div style="width: 61%; margin: 0;">
            <img src="static/figures/emergent_capabilities.png" width="100%" />
          </div>
          <p class="caption" style="width: 35%; margin-left: 4%;">
            <b>Figure.</b> <b>Increasing depth results in new capabilities:</b><br />
            <b><i>Row 1</i></b>: A humanoid agent trained with network depth 4 collapses and throws itself towards the
            goal, as opposed to in <b><i>Row 2</i></b>, where the depth 16 agent gains the ability to walk upright.
            <br /><b><i>Row 3</i></b>: At depth 64, the humanoid agent in U-Maze struggles to reach the goal and
            falls.
            <br /><b><i>Row 4</i></b>: An impressively novel policy emerges at depth 256, as the agent exhibits an
            acrobatic strategy of compressing its body to vault over the maze wall.
          </p>
        </div>

        <!-- Two column layout for remaining pairs -->
        <div class="columns bypad centered" style="width: 100%; margin: 0; display: flex;">
          <div style="width: 48%; margin-right: 2%;">
            <div style="width: 60%; margin: 0 auto;">
              <img src="static/figures/stitching.png" width="100%" />
            </div>
            <p class="caption">
              <b>Figure.</b> <b>Deeper networks exhibit improved generalization.</b><br />
              <i>(Top left)</i> We modify the training setup of the Ant U-Maze environment such that start-goal pairs
              are separated by ≤3 units. This design guarantees
              that no evaluation pairs <i>(top right)</i> were encountered during training, testing the ability for
              combinatorial generalization via "stitching."
              <br />
              <i>(Bottom)</i> Generalization ability improves as network depth grows from 4 to 16 to 64 layers.
            </p>
          </div>
          <div style="width: 48%; margin-left: 2%;">
            <div style="width: 70%; margin: 0 auto;">
              <img src="static/figures/Q_vis.png" width="100%" />
            </div>
            <p class="caption">
              <b>Figure.</b> <b>Deeper Q-functions are qualitatively different.</b><br />
              In the U4-Maze, the start and goal positions are indicated by the ⊙ and <b>G</b> symbols respectively, and
              the visualized Q values are computed via the L₂
              distance in the learned representation space, i.e., Q(s,a,g) = ‖φ(s,a) - ψ(g)‖₂. The shallow depth-4
              network <i>(left)</i> appears to naively rely on Euclidean proximity, exhibited by the high Q values of
              the semicircular gradient near the start position, despite the maze wall.
              <br />
              In the depth-64 heatmap <i>(right)</i>, the highest Q values cluster at the goal, gradually tapering along
              the maze's interior
              boundary. These results highlight how increasing depth is important for learning value functions in
              goal-conditioned settings, which are characterized by long horizons and sparse rewards.
            </p>
          </div>
        </div>
      </div>

      <hr style="width: 50%; margin: 40px auto; border: none; border-top: 1px solid #ddd;" />

      <h3 style="text-align: center; color: #27579a;">Scaling Width and Batch Size
      </h3>
      <!-- Two column layout for width and batch size -->
      <div class="columns bypad centered" style="width: 100%; margin: 0; display: flex;">
        <div style="width: 48%; margin-right: 2%;">
          <div style="width: 100%; margin: 0 auto;">
            <img src="static/figures/network_widths.png" width="100%" />
          </div>
          <p class="caption">
            <b>Figure.</b> <b>Scaling network width vs. depth.</b><br />
            We reflect findings from previous works which suggest that increasing network width can enhance
            performance. However, in contrast to prior work, our method is able to scale depth, yielding more impactful
            performance gains while also being more parameter-efficient (similar performance for $50\times$ smaller
            models). For instance, in the Humanoid environment, raising the width to 4096 (depth=4) fails to
            match the performance achieved by simply doubling the depth to 8 (width=256). This comparative advantage of
            scaling depth seems more pronounced as the observation dimensionality increases.
          </p>
        </div>
        <div style="width: 48%; margin-left: 2%;">
          <div style="width: 100%; margin: 0 auto;">
            <img src="static/figures/batch_sizes.png" width="100%" />
          </div>
          <p class="caption">
            <b>Figure.</b> <b>Deeper networks unlock batch size scaling.</b><br />
            Scaling batch size has been an effective mechanism in many areas in ML, but it has not demonstrated the same
            effectiveness in reinforcement learning. Our findings indicate that, while scaling batch size provides only
            marginal benefits at the original network capacity, larger networks can effectively leverage batch size
            scaling to achieve further improvements.
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>Summary of Key Empirical Findings</h2>
      <div class="findings-container">
        <ul class="key-findings">
          <li>
            CRL is scalable to depths unattainable by other RL proprioceptive algorithms (1000+ layers), perhaps due
            to
            its self-supervised nature.
          </li>
          <li>
            Both width and depth are key factors influencing CRL's performance, but depth achieves greater performance
            and better parameter-efficiency (similar performance for 50× smaller models).
          </li>
          <li>
            We observe signs of emergent behaviors in CRL with deep neural networks, such as humanoid learning to walk
            and navigate a maze.
          </li>
          <li>
            Scale unlocks learning difficult maze topologies.
          </li>
          <li>
            Batch size scaling occurs in CRL for deep networks.
          </li>
          <li>
            CRL benefits from both the actor and critic scale.
          </li>
        </ul>
      </div>

      <style>
        .findings-container {
          background-color: #f5f5f5;
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 20px;
          margin: 20px 0;
          width: 70%;
        }

        .key-findings {
          margin: 0;
          padding-left: 20px;
        }

        .key-findings li {
          margin-bottom: 10px;
        }

        .key-findings li:last-child {
          margin-bottom: 0;
        }
      </style>
    </section>

    <section>
      <h2>
        ${\bf B\kern-.05em{\small I\kern-.025em B}\kern-.08em
        T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}$
      </h2>
      <!--  <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@inproceedings{myers2025horizon,
    author    = {Myers, Vivek and Ji, Catherine and Eysenbach, Benjamin},
    booktitle = {{International Conference} on {Learning Representations}},
    title     = {{Horizon Generalization} in {Reinforcement Learning}},
    url       = {https://arxiv.org/abs/2501.02709},
    year      = {2025}, -->
      <pre class="language-bibtex bibtex" id="bibtex" onclick="selectContent('#bibtex')"><code>@article{wang2024thousand,
    title     = {1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities},
    author    = {Wang, Kevin and Javali, Ishaan and Bortkiewicz, Micha{\l} and Trzcinski, Tomasz and Eysenbach, Benjamin},
    journal   = {arXiv preprint arXiv:2501.02709},
    year      = {2024}
}</code></pre>
    </section>
  </main>
</body>

</html>